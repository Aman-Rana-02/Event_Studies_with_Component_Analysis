{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis (PCA) vs. Independent Component Analysis (ICA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Statistical analysis often entails extracting core structures from large datasets. In finance—or any discipline where numerous variables may mask underlying patterns—two widely recognized methods for **dimensionality reduction** are **Principal Component Analysis (PCA)** and **Independent Component Analysis (ICA)**. Both identify fewer components that summarize the original data, yet they do so with distinct objectives and assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "### 2.1 Goal and Concept\n",
    "\n",
    "**Principal Component Analysis** seeks linear transformations of a dataset into orthogonal directions (called *principal components*) that account for the greatest variance. The first principal component is the direction of maximal variance, the second is orthogonal to the first and explains the next largest slice of variance, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation\n",
    "\n",
    "Suppose there is a centered data matrix \n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times p},\n",
    "$$\n",
    "with $n$ observations and $p$ variables. Let $\\Sigma$ be the sample covariance matrix of $X$. The first principal component loading vector $\\mathbf{w}_1$ solves:\n",
    "$$\n",
    "\\mathbf{w}_1 = \\arg\\max_{\\|\\mathbf{w}\\|=1} \\mathbf{w}^\\top \\Sigma \\mathbf{w}.\n",
    "$$\n",
    "Subsequent loading vectors $\\mathbf{w}_k$ are chosen to be orthogonal to prior components and to maximize the remaining variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalue or SVD Perspective\n",
    "\n",
    "An equivalent approach is to compute the eigen-decomposition of the covariance matrix $\\Sigma$. The principal components then align with the eigenvectors, ranked by descending eigenvalues. Alternatively, a singular value decomposition (SVD) of $X$ reveals the same directions in a more computationally direct way when $n$ and $p$ are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Interpretation and Caveats\n",
    "\n",
    "- **Variance-Based**: PCA highlights the directions in which data vary most, providing a succinct summary of those variations.  \n",
    "- **Orthogonality vs. Independence**: The principal components are uncorrelated but not necessarily independent.  \n",
    "- **Sign Ambiguity**: A principal component may be multiplied by $-1$ without changing its explanatory power.  \n",
    "- **Caution**: A component can be a blend of multiple underlying signals if those signals align with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Independent Component Analysis (ICA)\n",
    "\n",
    "### 3.1 Goal and Concept\n",
    "\n",
    "**Independent Component Analysis** posits that observed variables are linear mixtures of distinct, statistically independent components. While PCA focuses on *maximizing variance* along uncorrelated directions, ICA aims to recover hidden signals that are *mutually independent* and *non-Gaussian*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation\n",
    "\n",
    "Assume\n",
    "$$\n",
    "X = A \\, S,\n",
    "$$\n",
    "where $X \\in \\mathbb{R}^{n \\times p}$ is observed data, $S \\in \\mathbb{R}^{n \\times p}$ contains independent source signals, and $A \\in \\mathbb{R}^{p \\times p}$ is an unknown mixing matrix. ICA seeks an unmixing matrix $W$ such that\n",
    "$$\n",
    "S = W \\, X\n",
    "$$\n",
    "yields maximally independent columns in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Maximizing Non-Gaussianity\n",
    "\n",
    "ICA relies on the principle that the sum (or mixture) of independent signals tends to appear more Gaussian than each original source (Central Limit Theorem). By optimizing a measure of *non-Gaussianity* (e.g., negentropy or kurtosis), ICA can separate out the sources. A popular algorithm, **FastICA**, uses contrast functions such as \n",
    "$$\n",
    "\\max \\Big|\\mathbb{E}[G(\\mathbf{w}^\\top X)] - \\mathbb{E}[G(v)]\\Big|,\n",
    "$$\n",
    "where $v$ is Gaussian and $G$ is a suitably chosen non-quadratic function (e.g., $\\log \\cosh$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Interpretation and Caveats\n",
    "\n",
    "- **Source Separation**: Each independent component can be viewed as a hidden signal that, when linearly combined, generates the observations.  \n",
    "- **Non-Gaussianity Requirement**: ICA performs best when sources deviate substantially from a Gaussian distribution.  \n",
    "- **Sensitivity**: ICA algorithms can be sensitive to initial guesses and the choice of contrast function.  \n",
    "- **Arbitrary Scaling and Signs**: Similar to PCA, ICA components can flip in sign or be scaled in ways that preserve independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparing PCA and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Criterion**         | **PCA**                                          | **ICA**                                           |\n",
    "|-----------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| **Objective**         | Maximize variance (decorrelation)               | Maximize statistical independence (source separation) |\n",
    "| **Components**        | Orthogonal (uncorrelated)                        | Statistically independent                         |\n",
    "| **Ranking**           | Ordered by explained variance                    | No inherent ordering by magnitude                |\n",
    "| **Statistical Assumption** | Primarily Gaussian-based variance measures | Non-Gaussian signals necessary for optimal separation |\n",
    "| **Robustness**        | Generally stable; closed-form (via eigen/SVD)    | Iterative, more sensitive to tuning              |\n",
    "| **Interpretation**    | Summarizes major modes of variation             | Reveals underlying \"independent\" structures      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Summary on PCA and ICA\n",
    "\n",
    "PCA and ICA are both powerful transformations that reduce dimensionality. Their difference lies in the objectives: PCA identifies directions of maximal variance and yields uncorrelated components, whereas ICA seeks statistically independent signals that may reveal latent structure in the data.\n",
    "\n",
    "In contexts where the goal is to summarize large-scale variation or reduce collinearity, PCA provides a statistical map—a structured overview of how and when variation occurs across dimensions. ICA, in contrast, can produce a behavioral fingerprint of the underlying generative processes, illuminating distinct patterns or \"archetypes\" that may not be visible through variance alone.\n",
    "\n",
    "PCA is best suited to quantifying how much and when variation occurs. ICA, when its assumptions are met, can offer a sharper picture of why that variation takes the shape it does—especially when hidden structure is believed to arise from distinct, independent sources.\n",
    "\n",
    "Both methods highlight the power—and responsibility—of statistical modeling. Each projects the same data onto a different conceptual lens, and each requires careful interpretation grounded in the assumptions it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Relevance in Event Studies\n",
    "\n",
    "Both PCA and ICA can be valuable tools in the context of **event studies**, where the objective is to detect systematic patterns in time-aligned data around specific occurrences—such as policy announcements, earnings releases, or macroeconomic shocks.\n",
    "\n",
    "In a typical event study setup, each observation corresponds to an event, and each variable represents a time offset relative to that event (e.g., days before and after an announcement). The resulting matrix encodes the temporal structure of returns, prices, or other metrics across many aligned events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of PCA\n",
    "\n",
    "PCA serves as a way to uncover **dominant modes of variation** in this matrix. The first few principal components may summarize the most common return patterns—such as gradual drifts, sharp reversals, or symmetric buildups and unwindings. These components are useful for identifying *where* in the event window variance is concentrated and for detecting whether certain features of the response are consistently amplified across events.\n",
    "\n",
    "Because PCA enforces orthogonality, it also acts as a decorrelating filter, which can aid in subsequent modeling (e.g., regression or classification) by reducing multicollinearity among features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of ICA\n",
    "\n",
    "ICA provides a complementary perspective. By emphasizing statistical independence rather than variance, it is capable of separating **distinct temporal motifs** that appear repeatedly but may not dominate in magnitude. In an event study, these may correspond to asymmetric reactions, delayed responses, or structural breaks that occur in only a subset of events but follow a common shape.\n",
    "\n",
    "Unlike PCA, ICA components are not ranked by explained variance. Instead, each one is interpreted as a distinct **latent signal** that—when linearly mixed—generates the observed responses. This property makes ICA especially appealing when the goal is to isolate interpretable behavioral or structural patterns embedded within noisy event-level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Considerations\n",
    "\n",
    "- Both methods assume **linearity** in the relationships between latent components and observed data.  \n",
    "- ICA further requires **non-Gaussianity** of the underlying sources, a condition often met in high-frequency or response-based financial data.  \n",
    "- Sign and scale remain indeterminate in both PCA and ICA; interpretation must rely on the **shape and structure** of components rather than their absolute direction or magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Figure placeholder: insert plots comparing PCA vs ICA components from illustrative notebook]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
