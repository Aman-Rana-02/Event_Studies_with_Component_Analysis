{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Component Analysis (PCA) vs. Independent Component Analysis (ICA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Statistical analysis often entails extracting core structures from large datasets. In finance—or any discipline where numerous variables may mask underlying patterns—two widely recognized methods for **dimensionality reduction** are **Principal Component Analysis (PCA)** and **Independent Component Analysis (ICA)**. Both identify fewer components that summarize the original data, yet they do so with distinct objectives and assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "### 2.1 Goal and Concept\n",
    "\n",
    "**Principal Component Analysis** seeks linear transformations of a dataset into orthogonal directions (called *principal components*) that account for the greatest variance. The first principal component is the direction of maximal variance, the second is orthogonal to the first and explains the next largest slice of variance, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation\n",
    "\n",
    "Suppose there is a centered data matrix \n",
    "$$\n",
    "X \\in \\mathbb{R}^{n \\times p},\n",
    "$$\n",
    "with $n$ observations and $p$ variables. Let $\\Sigma$ be the sample covariance matrix of $X$. The first principal component loading vector $\\mathbf{w}_1$ solves:\n",
    "$$\n",
    "\\mathbf{w}_1 = \\arg\\max_{\\|\\mathbf{w}\\|=1} \\mathbf{w}^\\top \\Sigma \\mathbf{w}.\n",
    "$$\n",
    "Subsequent loading vectors $\\mathbf{w}_k$ are chosen to be orthogonal to prior components and to maximize the remaining variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalue or SVD Perspective\n",
    "\n",
    "An equivalent approach is to compute the eigen-decomposition of the covariance matrix $\\Sigma$. The principal components then align with the eigenvectors, ranked by descending eigenvalues. Alternatively, a singular value decomposition (SVD) of $X$ reveals the same directions in a more computationally direct way when $n$ and $p$ are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Interpretation and Caveats\n",
    "\n",
    "- **Variance-Based**: PCA highlights the directions in which data vary most, providing a succinct summary of those variations.  \n",
    "- **Orthogonality vs. Independence**: The principal components are uncorrelated but not necessarily independent.  \n",
    "- **Sign Ambiguity**: A principal component may be multiplied by $-1$ without changing its explanatory power.  \n",
    "- **Caution**: A component can be a blend of multiple underlying signals if those signals align with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Independent Component Analysis (ICA)\n",
    "\n",
    "### 3.1 Goal and Concept\n",
    "\n",
    "**Independent Component Analysis** posits that observed variables are linear mixtures of distinct, statistically independent components. While PCA focuses on *maximizing variance* along uncorrelated directions, ICA aims to recover hidden signals that are *mutually independent* and *non-Gaussian*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical Formulation\n",
    "\n",
    "Assume\n",
    "$$\n",
    "X = A \\, S,\n",
    "$$\n",
    "where $X \\in \\mathbb{R}^{n \\times p}$ is observed data, $S \\in \\mathbb{R}^{n \\times p}$ contains independent source signals, and $A \\in \\mathbb{R}^{p \\times p}$ is an unknown mixing matrix. ICA seeks an unmixing matrix $W$ such that\n",
    "$$\n",
    "S = W \\, X\n",
    "$$\n",
    "yields maximally independent columns in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Maximizing Non-Gaussianity\n",
    "\n",
    "ICA relies on the principle that the sum (or mixture) of independent signals tends to appear more Gaussian than each original source (Central Limit Theorem). By optimizing a measure of *non-Gaussianity* (e.g., negentropy or kurtosis), ICA can separate out the sources. A popular algorithm, **FastICA**, uses contrast functions such as \n",
    "$$\n",
    "\\max \\Big|\\mathbb{E}[G(\\mathbf{w}^\\top X)] - \\mathbb{E}[G(v)]\\Big|,\n",
    "$$\n",
    "where $v$ is Gaussian and $G$ is a suitably chosen non-quadratic function (e.g., $\\log \\cosh$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Interpretation and Caveats\n",
    "\n",
    "- **Source Separation**: Each independent component can be viewed as a hidden signal that, when linearly combined, generates the observations.  \n",
    "- **Non-Gaussianity Requirement**: ICA performs best when sources deviate substantially from a Gaussian distribution.  \n",
    "- **Sensitivity**: ICA algorithms can be sensitive to initial guesses and the choice of contrast function.  \n",
    "- **Arbitrary Scaling and Signs**: Similar to PCA, ICA components can flip in sign or be scaled in ways that preserve independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparing PCA and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Criterion**         | **PCA**                                          | **ICA**                                           |\n",
    "|-----------------------|--------------------------------------------------|---------------------------------------------------|\n",
    "| **Objective**         | Maximize variance (decorrelation)               | Maximize statistical independence (source separation) |\n",
    "| **Components**        | Orthogonal (uncorrelated)                        | Statistically independent                         |\n",
    "| **Ranking**           | Ordered by explained variance                    | No inherent ordering by magnitude                |\n",
    "| **Statistical Assumption** | Primarily Gaussian-based variance measures | Non-Gaussian signals necessary for optimal separation |\n",
    "| **Robustness**        | Generally stable; closed-form (via eigen/SVD)    | Iterative, more sensitive to tuning              |\n",
    "| **Interpretation**    | Summarizes major modes of variation             | Reveals underlying \"independent\" structures      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Summary on PCA and ICA\n",
    "\n",
    "PCA and ICA are both powerful transformations that reduce dimensionality. Their difference lies in the objectives: PCA identifies directions of maximal variance and yields uncorrelated components, whereas ICA seeks statistically independent signals that may reveal latent structure in the data.\n",
    "\n",
    "In contexts where the goal is to summarize large-scale variation or reduce collinearity, PCA provides a statistical map—a structured overview of how and when variation occurs across dimensions. ICA, in contrast, can produce a behavioral fingerprint of the underlying generative processes, illuminating distinct patterns or \"archetypes\" that may not be visible through variance alone.\n",
    "\n",
    "PCA is best suited to quantifying how much and when variation occurs. ICA, when its assumptions are met, can offer a sharper picture of why that variation takes the shape it does—especially when hidden structure is believed to arise from distinct, independent sources.\n",
    "\n",
    "Both methods highlight the power—and responsibility—of statistical modeling. Each projects the same data onto a different conceptual lens, and each requires careful interpretation grounded in the assumptions it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Relevance in Event Studies\n",
    "\n",
    "Both PCA and ICA can be valuable tools in the context of **event studies**, where the objective is to detect systematic patterns in time-aligned data around specific occurrences—such as policy announcements, earnings releases, or macroeconomic shocks.\n",
    "\n",
    "In a typical event study setup, each observation corresponds to an event, and each variable represents a time offset relative to that event (e.g., days before and after an announcement). The resulting matrix encodes the temporal structure of returns, prices, or other metrics across many aligned events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of PCA\n",
    "\n",
    "PCA serves as a way to uncover **dominant modes of variation** in this matrix. The first few principal components may summarize the most common return patterns—such as gradual drifts, sharp reversals, or symmetric buildups and unwindings. These components are useful for identifying *where* in the event window variance is concentrated and for detecting whether certain features of the response are consistently amplified across events.\n",
    "\n",
    "Because PCA enforces orthogonality, it also acts as a decorrelating filter, which can aid in subsequent modeling (e.g., regression or classification) by reducing multicollinearity among features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of ICA\n",
    "\n",
    "ICA provides a complementary perspective. By emphasizing statistical independence rather than variance, it is capable of separating **distinct temporal motifs** that appear repeatedly but may not dominate in magnitude. In an event study, these may correspond to asymmetric reactions, delayed responses, or structural breaks that occur in only a subset of events but follow a common shape.\n",
    "\n",
    "Unlike PCA, ICA components are not ranked by explained variance. Instead, each one is interpreted as a distinct **latent signal** that—when linearly mixed—generates the observed responses. This property makes ICA especially appealing when the goal is to isolate interpretable behavioral or structural patterns embedded within noisy event-level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Considerations\n",
    "\n",
    "- Both methods assume **linearity** in the relationships between latent components and observed data.  \n",
    "- ICA further requires **non-Gaussianity** of the underlying sources, a condition often met in high-frequency or response-based financial data.  \n",
    "- Sign and scale remain indeterminate in both PCA and ICA; interpretation must rely on the **shape and structure** of components rather than their absolute direction or magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Outcome Examples**\n",
    "\n",
    "Look at the different return structures that can be found around FOMC meetings.\n",
    "![PC1 Returns](../figs/pc1_group_cumulative_returns.png)  \n",
    "![PC2 Returns](../figs/pc2_group_cumulative_returns.png)  \n",
    "![PC3 Returns](../figs/pc3_group_cumulative_returns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show us the different return structures around FOMC meetings.\n",
    "Principal components are ordinal, so PC1 captures the most prominent pattern in return behavior, PC2 the second most prominent, and so on.\n",
    "\n",
    "PC1 reveals a structure where returns tend to trend into the meeting and then flatten out afterward. This suggests that, in many cases, return movements leading up to the meeting are more pronounced than those after.\n",
    "\n",
    "PC2 highlights a pattern where returns continue to drift—either upward or downward—after the meeting. This may reflect instances where return adjustments are more extended or gradual.\n",
    "\n",
    "PC3 shows a pattern where returns move in one direction before the meeting and then reverse afterward, with flat regions in between. This structure could reflect periods of return uncertainty or re-alignment around the meeting date.\n",
    "\n",
    "Note: The outcomes are about general repetitive patterns since this is a PCA interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ICA Outcome Examples**\n",
    "\n",
    "![ICA1 Returns](../figs/ic1_group_cumulative_returns.png)\n",
    "![ICA2 Returns](../figs/ic2_group_cumulative_returns.png)\n",
    "![ICA3 Returns](../figs/ic3_group_cumulative_returns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike PCA, which decomposes based on variance and orthogonal patterns, ICA identifies statistically independent temporal structures, allowing us to isolate distinct return dynamics tied to different types of meetings.\n",
    "\n",
    "IC1 Meetings exhibit strong downward drift in market returns after the meeting, whereas those with low IC1 loadings show rising returns post-meeting. This suggests that IC1 captures an independent post-meeting response structure, where some meetings are associated with negative sentiment or surprise, and others with positive reassessment or relief. The component is directional and polarizes post-meeting return paths.\n",
    "\n",
    "IC2 Meetings show a pattern where returns drift positively into the meeting and then flatten out, while low IC2 loadings show negative pre-meeting drift, also flattening after. This indicates IC2 captures an anticipatory structure in market behavior—where pricing dynamics are primarily driven by expectations leading up to the meeting, and the meeting itself leads to minimal additional movement, this indicates that the market has no surprise or encouragement, it perfectly prices the meeting.\n",
    "\n",
    "IC3 Meetings show return movement before the meeting, but in opposite directions, followed by very little post-meeting activity. This component appears to isolate a structure of pre-meeting uncertainty resolution, where the market forms a directional view very far in advance, and the meeting itself has no affect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ICA offers a different interpretive lens:\n",
    "\n",
    "Because ICA extracts **statistically independent drivers** of return structure, we can interpret components in terms of **distinct temporal behaviors** around FOMC events—not just patterns of co-movement. This allows us to use language such as:\n",
    "\n",
    "- **“response structures”**  \n",
    "- **“anticipatory dynamics”**  \n",
    "- **“independent behavioral signatures”**  \n",
    "\n",
    "These phrases reflect the idea that ICA components may correspond to **underlying mechanisms** or **behavioral processes** in how markets react to monetary policy events. In other words, ICA gives us a framework to describe *how different types of meetings affect market returns differently*.\n",
    "\n",
    "---\n",
    "\n",
    "### In contrast, with PCA:\n",
    "\n",
    "PCA components are **orthogonal linear combinations** of the data, ranked by how much **total variance** they explain. This means we are limited to describing **statistical structure**, not behavioral meaning. Interpretations must focus on:\n",
    "\n",
    "- **“common patterns of variation”**  \n",
    "- **“dominant return structures”**  \n",
    "- **“explained variance in return trajectories”**\n",
    "\n",
    "PCA does not allow us to talk about **drivers**, **sources**, or **behavioral signals**, because its components are **mathematically constrained** to be uncorrelated and optimized purely for variance—not independence or interpretability. For example, in PCA we might say:\n",
    "\n",
    "> “PC1 captures the most prominent co-movement in returns around meetings.”\n",
    "\n",
    "But we would avoid saying:\n",
    "\n",
    "> ❌ “PC1 captures a behavioral response to FOMC outcomes.”\n",
    "\n",
    "Because PCA doesn’t isolate cause or independence—only structure.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **PCA tells us what the most prominent *shapes* of return movement are.**  \n",
    "- **ICA allows us to ask what *distinct market behaviors* may underlie those movements.**\n",
    "\n",
    "Both are powerful tools—but ICA opens the door to more meaningful event-driven interpretations, especially when analyzing market reactions to scheduled, high-impact events like FOMC meetings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
